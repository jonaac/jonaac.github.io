<!DOCTYPE html>
<html>

<head>
	<title>Jona</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<!-- style / responsive -->
	<link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css" />
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
	<link rel="stylesheet" type="text/css" href="../css/style.css">

	<!-- fonts -->
	<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	<!-- icons -->
	<link href="../assets/fonts-icons/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<!-- for FF, Chrome, Opera -->
	<link rel="icon" type="image/png" href="" sizes="16x16">
	<link rel="icon" type="image/png" href="" sizes="32x32">

	<!-- for IE -->
	<link rel="icon" type="image/x-icon" href="" >
	<link rel="shortcut icon" type="image/x-icon" href=""/>

</head>
<body>

	<header class="wrapper site-header">
		<div class="row">
			<hgroup>
				<h1 class="site-title">
					<a href="../index.html"> Jona </a>
				</h1>

			</hgroup>
			<!-- Navigation Bar -->
			<nav class="navbar navbar-static-top center" role="navigation">
				<div class="navbar-header ">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-menubuilder">	
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
				</div>
				<div class="collapse navbar-collapse navbar-menubuilder navbar-inner">
					<ul class="nav navbar-nav">
						<li><a href="../index.html" class="page-scroll">Projects</a></li>
						<li><a href="../aboutme.html" class="page-scroll">About Me</a></li>
						<li><a href="../thesis.html" class="page-scroll">Thesis </a></li>
						<li><a href="../contact.html" class="page-scroll">Contact </a></li>
						<!--<li><a class="page-scroll" href="#"><b> BLOG </b></a></li>-->
					</ul>
				</div>
			</nav>
			<!-- /Navigation Bar -->
		</div>
	</header>

	<section class="middle wrapper">
		<div class="row">
			<div id="primary" class="site-content">
				<!-- #content -->
				<div id="content" role="main">
					<div class="blog-posts readable-content">
						<!-- .post -->
						<article class="post type-post hentry">
							<!-- .entry-header -->
							<header class="entry-header">
								<h1 class="entry-title">
									<a href="#" title="Permalink to Adaptive Vs. Responsive Layouts And Optimal Form Field Labels" rel="bookmark">An Introduction to Safe Reinforcement Learning</a>
								</h1>
							</header>
							<!-- .entry-header -->
							 
							 <!-- .entry-meta --> 
							<footer class="entry-meta">
								<ul>
									<li><p href="#">AI Ethics</p></li>
									<li><p href="#">Q-Learning</p></li>
									<li><p href="#">Python</p></li>
									<li>
										<p href="#">
											<a href="https://github.com/jonaac/Reinforcement-Learning-Safety-Project">
												<i class="fa fa-github"></i>
											</a>
										</p>
									</li>
								</ul>			
							</footer>

							<div class="featured-image">
								<img src="../img/marsrover.jpg" alt="blog-image">
							</div>
							<!-- .entry-meta --> 
							
							<!-- .entry-content -->
							<div class="entry-content work">

								<p class="abstract"> With Reinforcement Learning's (RL) increases in popularity, the more research there is concerning the safety of RL algorithms. Both considering the agent's safety and the safety of any element in the agent's environment (objects, humans, animals, etc.). This project is my introduction to RL and safe RL. I develop two grid-world simulation and RL algorithms from scratch, with no ML or RL libraries, to experiment and test safety methods. After training and deploying the agents, I was able to achieve my goal. For every environment, the agent was capable of learning an optimal policy while staying safe. <a href="https://github.com/jonaac/Reinforcement-Learning-Safety-Project" class="more-link">Github Reposiroty <span class="meta-nav"> â†’</span></a>
								</p>

								<h2 style="text-align: center">An Introduction to Safe Reinforcement Learning</h2>

								<p class="article">The use of Reinforcement Learning (RL) is becoming more widespread. With RL's rise in popularity, we have also seen a growing interest in studying the safety of RL algorithms. On one hand, we have the safety of the learning agent during training. On the other hand, we want to ensure that the agent is safe to deploy. For example, we want to safely train an autonomous car and ensure that it won't be responsible for any accidents when deployed.</p>

								<div class="featured-image">
									<img src="../img/tesla.jpg" alt="blog-image">
								</div>
								<footer class="entry-meta">
									Example of an autnomous car safely deployed on a urban environment.
								</footer>

								<p class="article">Safe Reinforcement learning has its complexities. Given that RL is based on exploring and experiencing, how can an agent learn to be safe without experiencing unsafe scenarios? Furthermore, for some tasks, the threat of safety may be very infrequent. It is hard to ensure that training sets include enough unsafe states to ensure that safety-preserving behaviors are learned. Also, some reinforcement learning techniques construct their own representations of the domain (e.g., deep learning). It is hard to ensure that these representations carry the necessary information to ensure safety and that the resulting policies/strategies are safe.</p>

								<p class="article">Reinforcement learning and Safe RL are fields of study that I am very interested in. After learning more about RL [<a href="http://incompleteideas.net/book/the-book.html" >1</a>] and the work being done on safe RL [<a href="https://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf" >2</a>], I thought it would be a good idea to develop some algorithms myself. For this project, I develop two grid-world environments and an RL algorithm. Everything is developed from scratch, meaning that I didn't use any ML or RL libraries.

								<p class="article">Both environments involve 5x4 grids limited by four borders. The agent can move from one grid to another by performing 1 out of 4 possible actions (Up, Down, Left, and Right). The first set of grid-world environments are Navigation simulations. It consists of a planetary robot looking to travel from a starting point to a specified target destination while trying not to fall into craters.</p>

								<p class="article">The second set of grid-world environments are Mineral Collection simulations. It consists of a planetary robot looking to collect mineral ore and bringing it back to its starting position while trying not to fall into craters. The agent will automatically pick up the ore when it reaches its location. </p>

								<div class="featured-image list">
									<ul>
										<li><img src="../img/grid-1.jpg" alt="blog-image"></li>
										<li><img src="../img/mineral-1.jpg" alt="blog-image"></li>
									</ul>
								</div>
								<footer class="entry-meta">
									<b>Navigation</b>, We have the agent/planetary robot (blue), the starting point (purple), the target (green) and the craters (red). <b>Mineral Collection</b>, We have the agent/planetary robot (blue), the start & end point (purple), the mineral ore (yellow) and the craters (red).
								</footer>

								<p class="article">I implemented the Q-learning algorithm presented in Richard Sutton's textbook [<a href="http://incompleteideas.net/book/the-book.html" >1</a>]. It helped me better understand Reinforcement Learning and the difference between off-policy and on-policy learning. The Q-learning algorithm should be able to solve any discrete RL problem. It only requires that the MDP has a finite set of states, a finite set of actions, a set of rewards, and a transition matrix.</p>

								<div class="featured-image">
									<img src="../img/qlearning.png" alt="blog-image">
								</div>
								<footer class="entry-meta">
									Pseudo-Code for the Q-learning algorithm.
								</footer>

								<p class="article"> In terms of safety, for this project, I focused on making sure that the agent is capable of learning a policy that is safe to deploy. More specifically, make sure that the agent will not fall into any crater on deployment. In our MDP the unsafe states will be represented as states with negative rewards associated with them. The hypothesis is that by maximizing the expected reward the agent will avoid those states that provide a negative reward. For both simulations, the craters have a negative reward, and the target destination has a positive reward. All other states have a reward of 0. In the Mineral Collection environment picking up the mineral ore also has a positive reward. </p>

								<p class="article">After developing, training, and deploying the agents I was able to achieve my goal. For every iteration of the Navigation and Mineral Collection environments, the agent learned an optimal policy without falling into any crater. The key was to test different values for the rewards, and parameter tuning the Q-learning algorithm. If the punishment was too high, the agent would mainly focus on not falling and would avoid its actual goal or it would learn a non-optimal policy. If the reward was too high the agent would ignore the craters and the resulting deployment would be very unsafe.</p>

								<div class="featured-image list">
									<ul>
										<li><img src="../img/mineral.gif" alt="blog-image"></li>
										<li><img src="../img/navigation.gif" alt="blog-image"></li>
									</ul>
								</div>
								<footer class="entry-meta">
									The agent deployed in both environments after training with Q-learning algorithm.
								</footer>

								<p class="article">Even though my approach works, it had a lot to do with manually tunning several parameters to succeed. As mentioned earlier, this project is my introduction to Reinforcement Learning and Safe RL. In the future, I will be focusing on more complex safety strategies for RL and work on the agent's safety during training.</p>

							</div>
							<!-- .entry-content -->
						</article>
					</div>
						<!-- .blog-posts -->
				</div>
				<!-- #content --> 
			</div>
		</div>
	</section>

	<script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="../js/vendors/scrolloverflow.min.js"></script>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
	<script type="text/javascript" src="../js/script.js"></script>
</body>

</html>
