<!DOCTYPE html>
<html>

<head>
	<title>Jona</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<!-- style / responsive -->
	<link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css" />
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
	<link rel="stylesheet" type="text/css" href="../css/style.css">

	<!-- fonts -->
	<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	<!-- icons -->
	<link href="../assets/fonts-icons/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
	<!-- for FF, Chrome, Opera -->
	<link rel="icon" type="image/png" href="" sizes="16x16">
	<link rel="icon" type="image/png" href="" sizes="32x32">

	<!-- for IE -->
	<link rel="icon" type="image/x-icon" href="" >
	<link rel="shortcut icon" type="image/x-icon" href=""/>

</head>
<body>

	<header class="wrapper site-header">
		<div class="row">
			<hgroup>
				<h1 class="site-title">
					<a href="../index.html"> Jona </a>
				</h1>
				<h2 class="site-description">
					Machine Learning Engineer
				</h2>
			</hgroup>
			<!-- Navigation Bar -->
			<nav class="navbar navbar-static-top center" role="navigation">
				<div class="navbar-header ">
					<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-menubuilder">	
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
				</div>
				<div class="collapse navbar-collapse navbar-menubuilder navbar-inner">
					<ul class="nav navbar-nav">
						<li><a href="../index.html" class="page-scroll">Projects</a></li>
						<li><a href="../aboutme.html" class="page-scroll">About Me</a></li>
						<li><a href="../thesis.html" class="page-scroll">Thesis </a></li>
						<li><a href="../contact.html" class="page-scroll">Contact </a></li>
						<!--<li><a class="page-scroll" href="#"><b> BLOG </b></a></li>-->
					</ul>
				</div>
			</nav>
			<!-- /Navigation Bar -->
		</div>
	</header>

	<section class="middle wrapper">
		<div class="row">
			<div id="primary" class="site-content">
				<!-- #content -->
				<div id="content" role="main">
					<div class="blog-posts readable-content">
						<!-- .post -->
						<article class="post type-post hentry">
							<!-- .entry-header -->
							<header class="entry-header">
								<h1 class="entry-title">
									<a href="#" title="Permalink to Adaptive Vs. Responsive Layouts And Optimal Form Field Labels" rel="bookmark">An Introduction to Safe Reinforcement Learning</a>
								</h1>
							</header>
							<!-- .entry-header -->
							 
							 <!-- .entry-meta --> 
							<footer class="entry-meta">
								<ul>
									<li><p href="#">AI Ethics</p></li>
									<li><p href="#">Q-Learning</p></li>
									<li><p href="#">Python</p></li>
									<li>
										<p href="#">
											<a href="https://github.com/jonaac/Reinforcement-Learning-Safety-Project">
												<i class="fa fa-github"></i>
											</a>
										</p>
									</li>
								</ul>			
							</footer>

							<div class="featured-image">
								<img src="../img/marsrover.jpg" alt="blog-image">
							</div>
							<!-- .entry-meta --> 
							
							<!-- .entry-content -->
							<div class="entry-content work">

								<p class="abstract"> With Reinforcement Learning's (RL) increases in popularity, the more research we have been seeing concerning the safety of RL algorithms. Both in regards to the agent's safety and the safety of any element in the agent's environment (objects, humans, animals, etc.). This project is my introduction to RL and safe RL. I develop two grid-world simulation and RL algorithms from scratch, no ML or RL libraries, to experiment and test safety methods. After developing, training and deploying the agents I was able to achieve my goal. For every environment the agent was able to learn an optimal policy (shortest route) without falling into any crater. <a href="https://github.com/jonaac/Reinforcement-Learning-Safety-Project" target="_blank" class="more-link">Github Reposiroty <span class="meta-nav"> â†’</span></a>
								</p>

								<h2 style="text-align: center">An Introduction to Safe Reinforcement Learning</h2>

								<p class="article">The use of Reinforcement Learning (RL) techniques to build autonomous agents and systems is becoming more widespread. With the rise in popularity we have also seen a growing interest in studying safety concern with relation to RL algorithms. On one hand, we have the safety of the learning agent during training, on the other hand we want to ensure that the agent is capable to learn policies that are safe to deploy. For example, in simple terms, we want to be able to safely train an autonomous car and ensure that once it's trained it won't be responsible for any accidents.</p>

								<div class="featured-image">
									<img src="../img/tesla.jpg" alt="blog-image">
								</div>
								<footer class="entry-meta">
									Example of an autnomous car safely deployed on a urban environment.
								</footer>

								<p class="article">Safe Reinforcement learning has it's own complexity. Given that RL is based on learning by exploring and experiencing, how can an agent learn to be safe without experiecing unsafe scenarios? Furthermore, for some tasks, threat of safety may be very infrequent and it is hard to ensure that the training set includes enough of these to ensure that safety-preserving behaviors are learned. Also, some reinfocement learning techniques construct their own representations of the domain (e.g., deep learning) and it is hard to ensure that these representations carry the necessary information to ensure safety, and that the resulting policies/strategies are safe.</p>

								<p class="article">Reinforcement learning and Safe RL are field of study that I am very intrested in. After reading some theory [<a href="http://incompleteideas.net/book/the-book.html" target="_blank">1</a>] on RL and learning about the work being done on safe RL [<a href="https://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf" target="_blank">2</a>] I thought it would be a good idea to develop some algorithms myself. For this project I develop two grid-world environments and a RL algorithm for each environment. Everything is developed from scratch, meaning that I didn't use any ML or RL libraries.</p>

								<p class="article">Both environments involve 5x4 grids limited by four borders. The agent will be able to move from one grid to another by performing 1 out of 4 possible actions (Up, Down, Left and Right). The first set of grid-world environments are Navigation simulations. It consists of a 'planetary robot' looking travel from a starting point to a specified target destination while avoiding to fall into 'craters'.</p>

								<p class="article">The second set of grid-world environments are Mineral Collection simulations. It consists of a planetary robot looking to collect mineral ore and bringing it back to its starting position while avoiding to fall into 'craters'. In the case of the Mineral Colection environment the agent will automatically pick up the ore when it reaches it's position.</p>

								<div class="featured-image list">
									<ul>
										<li><img src="../img/grid-1.jpg" alt="blog-image"></li>
										<li><img src="../img/mineral-1.jpg" alt="blog-image"></li>
									</ul>
								</div>
								<footer class="entry-meta">
									<b>Navigation</b>, We have the agent/planetary robot (blue), the starting point (purple), the target (green) and the craters (red). <b>Mineral Collection</b>, We have the agent/planetary robot (blue), the start & end point (purple), the mineral ore (yellow) and the craters (red).
								</footer>

								<p class="article">I implemented the Q-learning algorithm presented in Richard Sutton's textbook [<a href="http://incompleteideas.net/book/the-book.html" target="_blank">1</a>]. It helped me better understand Reinforcement Learning and the difference between off-policy and on-policy learning. I wanted to develop a single Q-learning algorithm that could be used to solve any RL problem, as long as the problem trying to be solved is performed in an environment with a finite set of states (including an initial state and a goal state), a finite set of actions, a set of rewards and a transition matrix.</p>

								<div class="featured-image">
									<img src="../img/qlearning.png" alt="blog-image">
								</div>
								<footer class="entry-meta">
									Pseudo-Code for the Q-learning algorithm.
								</footer>

								<p class="article"> In terms of safety, for this project I focused on making sure that the agent is capable of learning a policy that is safe to deploy. More specifically, make sure that the agent will not fall into any crater on deployment. In our MDP the unsafe states will be represented as states with negative reward associated to them. The hypothesis is that by looking maximizing the expected reward the agent will avoid those states that provide a negative reward. For both simulations the craters have a negative reward, and the target destination has a positive reward, all other states have a reward of 0. In the Mineral Collection environment picking up the mineral ore aslo has a positive reward.</p>

								<p class="article">After developing, training and deploying the agents I was able to achieve my goal. For every iteration of the Navigation and Mineral Collection environments the agent was able to learn an optimal policy (shortest route) without falling into any crater. The key was to test different values for the positive and negative reward, and parameter tune the Q-learning algorithm. If the punishment was too high, the agent would mainly focus on not falling and would avoid its actual goal or it would learn a non-optimal policy. If the reward was too high the agent wouldn't take the unsafe state into consideration and the resulting deployment would be very un-safe.</p>

								<div class="featured-image list">
									<ul>
										<li><img src="../img/mineral.gif" alt="blog-image"></li>
										<li><img src="../img/navigation.gif" alt="blog-image"></li>
									</ul>
								</div>
								<footer class="entry-meta">
									The agent deployed in both environments after training with Q-learning algorithm.
								</footer>

								<p class="article">Even though my approach works, it had a lot to do with manually tunning several parameters in order to succeed, this is far from ideal. As mentioned earlier this is my introduction to Reinforcement Learning and Safe RL. In the future I will be focusing on more complex safety strategies for RL and work on the agent's safety during training.</p>

							</div>
							<!-- .entry-content -->
						</article>
					</div>
						<!-- .blog-posts -->
				</div>
				<!-- #content --> 
			</div>
		</div>
	</section>

	<script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="../assets/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="../js/vendors/scrolloverflow.min.js"></script>

	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
	<script type="text/javascript" src="../js/script.js"></script>
</body>

</html>
